{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG in Action - Part 1: Naive RAG Demo\n",
    "\n",
    "A complete demonstration of building a basic RAG system with Apple's 2023 10-K filing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: /Users/kakao/Dev/personal/medium/RAG in Action/rag-in-action-series/part_01/data/apple_10k_2023.pdf\n",
      "Downloaded: /Users/kakao/Dev/personal/medium/RAG in Action/rag-in-action-series/part_01/data/apple_10k_2023.pdf\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "\n",
    "from data.download_data import download_apple_10k\n",
    "\n",
    "# Download Apple 10-K report\n",
    "file_path = download_apple_10k()\n",
    "print(f\"Downloaded: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test LLM Limitations (Before RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö´ LLM WITHOUT RAG\n",
      "==================================================\n",
      "Question: What was Apple's total revenue in 2023? Please provide the exact number.\n",
      "Answer: I don't have information on Apple's revenue for 2023 as I'm a large language model, my training data only goes up to 2022 and does not include real-time updates or future financial data.\n",
      "\n",
      "However, you can check Apple's official investor relations website (investors.apple.com) for the most recent financial reports. They release their annual reports around late January of each year.\n",
      "Response time: 4.06s\n",
      "Source: ‚ùå None (relies on parameter memory only)\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import time\n",
    "\n",
    "question = \"What was Apple's total revenue in 2023? Please provide the exact number.\"\n",
    "\n",
    "print(\"üö´ LLM WITHOUT RAG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "response = ollama.chat(\n",
    "    model='llama3.1:8b',\n",
    "    messages=[{'role': 'user', 'content': question}]\n",
    ")\n",
    "llm_time = time.time() - start_time\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response['message']['content']}\")\n",
    "print(f\"Response time: {llm_time:.2f}s\")\n",
    "print(f\"Source: ‚ùå None (relies on parameter memory only)\")\n",
    "\n",
    "llm_response = response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loaded 80 pages\n",
      "Created 358 chunks\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Check device availability\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} pages\")\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Embeddings and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "print(\"üì• Loading embedding model...\")\n",
    "print(\"‚è∞ First run may take 1-2 minutes for model download\")\n",
    "\n",
    "# Initialize embeddings with Mac M1 optimization\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded!\")\n",
    "print(\"üîÑ Creating vector store...\")\n",
    "\n",
    "# Create vector store\n",
    "start_time = time.time()\n",
    "\n",
    "vector_store = Qdrant.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"apple_10k\"\n",
    ")\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Vector store created in {embedding_time:.2f}s\")\n",
    "print(f\"üìä Stored {len(chunks)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ RAG SYSTEM WITH CONTEXT\n",
      "==================================================\n",
      "üîç Found 5 relevant documents\n",
      "Question: What was Apple's total revenue in 2023? Please provide the exact number.\n",
      "Answer: According to the information provided, Apple's total net sales (revenue) for 2023 were $383.285 billion.\n",
      "Response time: 3.44s\n",
      "Source: ‚úÖ 5 documents from 2023 10-K filing\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚úÖ RAG SYSTEM WITH CONTEXT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "print(f\"üîç Found {len(docs)} relevant documents\")\n",
    "\n",
    "# Combine retrieved context\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Create RAG prompt\n",
    "prompt = f\"\"\"\n",
    "Based on the following information from Apple's 2023 10-K filing, answer the question accurately:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Please provide a specific answer based on the provided context.\n",
    "\"\"\"\n",
    "\n",
    "# Generate RAG response\n",
    "start_time = time.time()\n",
    "rag_response = ollama.chat(\n",
    "    model='llama3.1:8b',\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "rag_time = time.time() - start_time\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_response['message']['content']}\")\n",
    "print(f\"Response time: {rag_time:.2f}s\")\n",
    "print(f\"Source: ‚úÖ {len(docs)} documents from 2023 10-K filing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nüìä COMPARISON RESULTS\")\nprint(\"=\" * 60)\n\nprint(\"\\n‚ùå LLM WITHOUT RAG:\")\nprint(f\"   {llm_response}\")\nprint(f\"   ‚è±Ô∏è  Response time: {llm_time:.2f}s\")\nprint(f\"   üìÑ Source: None\")\nprint(f\"   üéØ Reliability: Low (no verification possible)\")\n\nprint(\"\\n‚úÖ LLM WITH RAG:\")\nprint(f\"   {rag_response['message']['content']}\")\nprint(f\"   ‚è±Ô∏è  Response time: {rag_time:.2f}s\")\nprint(f\"   üìÑ Source: {len(docs)} documents\")\nprint(f\"   üéØ Reliability: High (traceable sources)\")\n\nprint(\"\\nüìö RETRIEVED SOURCES:\")\nfor i, doc in enumerate(docs[:3], 1):\n    page_num = doc.metadata.get('page', 'Unknown')\n    print(f\"   Source {i} (Page {page_num}): {doc.page_content[:100]}...\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üí° Why did this difference occur?\")\nprint(\"=\"*60)\nprint(\"\"\"\nDid you see that? The pure LLM without RAG avoided answering due to lack of recent information,\nbut the RAG-applied system accurately referenced the 2023 10-K report we provided and\ngenerated answers with specific figures.\n\nWe can even trace the documents that served as the basis for the answers.\nThis is the power of RAG!\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Additional Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ADDITIONAL TEST QUESTIONS\n",
      "==================================================\n",
      "\n",
      "‚úÖ Success Cases:\n",
      "\n",
      "1. What are Apple's main business segments?\n",
      "   Answer: According to the text, Apple's main business segments are:\n",
      "\n",
      "1. Products:\n",
      "\t* Smartphones (iPhone line)\n",
      "\t* Personal computers (Mac line)\n",
      "\t* Tablets\n",
      "\t* W...\n",
      "\n",
      "2. What was Apple's gross margin in 2023?\n",
      "   Answer: Unfortunately, the provided text does not mention Apple's gross margin for 2023.\n",
      "\n",
      "However, we can infer that to find the gross margin, we would need t...\n",
      "\n",
      "3. What are the main risk factors for Apple?\n",
      "   Answer: Based on Apple's 2023 10-K filing, the main risk factors for Apple include:\n",
      "\n",
      "1. **Quality and product reliability issues**: Failure to detect and fix ...\n",
      "\n",
      "\n",
      "‚ùå Limitation Demonstration:\n",
      "Question: What was the market reaction to Apple Vision Pro's initial sales volume?\n",
      "Answer: There is no information in the provided text about Apple Vision Pro's initial sales volume or market reaction. The text does mention that Apple Vision Pro, a spatial computer featuring visionOS, \"is expected to be available in early calendar year 2024\", but it does not provide any details on its actual availability, sales, or market performance.\n",
      "\n",
      "============================================================\n",
      "üö® Did you see the limitations of Naive RAG?\n",
      "============================================================\n",
      "\n",
      "The 2023 10-K report doesn't contain specific Vision Pro sales data.\n",
      "Naive RAG shows limitations when:\n",
      "- Keywords don't match exactly, or\n",
      "- Asked about content not in the documents\n",
      "\n",
      "In Part 2 and 3, we'll solve these problems by introducing:\n",
      "üîπ Advanced Qdrant filtering\n",
      "üîπ Hybrid search (semantic + keyword)  \n",
      "üîπ Query rewriting techniques\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test additional questions\n",
    "test_questions = [\n",
    "    \"What are Apple's main business segments?\",\n",
    "    \"What was Apple's gross margin in 2023?\",\n",
    "    \"What are the main risk factors for Apple?\"\n",
    "]\n",
    "\n",
    "def ask_rag(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Based on Apple's 2023 10-K filing:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model='llama3.1:8b',\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "print(\"\\nüîç ADDITIONAL TEST QUESTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ Success Cases:\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. {question}\")\n",
    "    answer = ask_rag(question)\n",
    "    print(f\"   Answer: {answer[:150]}...\")\n",
    "\n",
    "# Demonstrate limitations\n",
    "failure_question = \"What was the market reaction to Apple Vision Pro's initial sales volume?\"\n",
    "\n",
    "print(\"\\n\\n‚ùå Limitation Demonstration:\")\n",
    "print(f\"Question: {failure_question}\")\n",
    "failure_answer = ask_rag(failure_question)\n",
    "print(f\"Answer: {failure_answer}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üö® Did you see the limitations of Naive RAG?\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "The 2023 10-K report doesn't contain specific Vision Pro sales data.\n",
    "Naive RAG shows limitations when:\n",
    "- Keywords don't match exactly, or\n",
    "- Asked about content not in the documents\n",
    "\n",
    "In Part 2 and 3, we'll solve these problems by introducing:\n",
    "üîπ Advanced Qdrant filtering\n",
    "üîπ Hybrid search (semantic + keyword)  \n",
    "üîπ Query rewriting techniques\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test additional questions\n",
    "test_questions = [\n",
    "    \"What are Apple's main business segments?\",\n",
    "    \"What was Apple's gross margin in 2023?\",\n",
    "    \"What are the main risk factors for Apple?\"\n",
    "]\n",
    "\n",
    "def ask_rag(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Based on Apple's 2023 10-K filing:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model='llama3.1:8b',\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "print(\"\\nüîç ADDITIONAL TEST QUESTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ Success Cases:\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. {question}\")\n",
    "    answer = ask_rag(question)\n",
    "    print(f\"   Answer: {answer[:150]}...\")\n",
    "\n",
    "# Demonstrate limitations\n",
    "failure_question = \"What was the market reaction to Apple Vision Pro's initial sales volume?\"\n",
    "\n",
    "print(\"\\n\\n‚ùå Limitation Demonstration:\")\n",
    "print(f\"Question: {failure_question}\")\n",
    "failure_answer = ask_rag(failure_question)\n",
    "print(f\"Answer: {failure_answer}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üö® Did you see the limitations of Naive RAG?\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "The 2023 10-K report doesn't contain specific Vision Pro sales data.\n",
    "Naive RAG shows limitations when:\n",
    "- Keywords don't match exactly, or\n",
    "- Asked about content not in the documents\n",
    "\n",
    "In Part 2 and 3, we'll solve these problems by introducing:\n",
    "üîπ Advanced Qdrant filtering\n",
    "üîπ Hybrid search (semantic + keyword)  \n",
    "üîπ Query rewriting techniques\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Implementation Note:\nWe implemented RAG manually step-by-step for educational purposes. LangChain provides higher-level abstractions like `RetrievalQA` chains that can make this process more concise, but understanding the underlying mechanics helps you customize and debug your RAG systems more effectively.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "üéâ **You've successfully built your first RAG system!**\n",
    "\n",
    "### What we accomplished:\n",
    "- ‚úÖ Downloaded and processed Apple's 2023 10-K filing\n",
    "- ‚úÖ Implemented document chunking strategy  \n",
    "- ‚úÖ Created embeddings using free, local models\n",
    "- ‚úÖ Set up Qdrant vector database\n",
    "- ‚úÖ Built a complete RAG pipeline\n",
    "- ‚úÖ Demonstrated clear improvements over vanilla LLM\n",
    "\n",
    "### Key improvements with RAG:\n",
    "1. **Factual accuracy** - Based on real documents\n",
    "2. **Source traceability** - Can verify information\n",
    "3. **Up-to-date information** - Uses latest 2023 data\n",
    "4. **Reduced hallucination** - Grounded in provided context\n",
    "5. **Domain expertise** - Specialized financial knowledge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}