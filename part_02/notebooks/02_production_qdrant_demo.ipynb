{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RAG in Action - Part 2: Production Qdrant Demo\n",
    "\n",
    "Building production-grade RAG with Docker Qdrant, metadata filtering, and persistent vector storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 0: Start Qdrant Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nw7eezjifj",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def start_qdrant_server():\n",
    "    \"\"\"One-click Qdrant server launch with Docker Compose\"\"\"\n",
    "    print(\"ğŸš€ Starting Qdrant server...\")\n",
    "    \n",
    "    # Run Docker Compose\n",
    "    result = subprocess.run(\n",
    "        [\"docker-compose\", \"up\", \"-d\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… Qdrant server started!\")\n",
    "        \n",
    "        # Wait for connection\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                response = requests.get(\"http://localhost:6333\")\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"ğŸŒ Qdrant server connected successfully! (http://localhost:6333)\")\n",
    "                    return True\n",
    "            except:\n",
    "                print(f\"â³ Waiting for connection... ({i+1}/10)\")\n",
    "                time.sleep(2)\n",
    "    \n",
    "    print(\"âŒ Server startup failed\")\n",
    "    return False\n",
    "\n",
    "# Start Qdrant server\n",
    "server_ready = start_qdrant_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 1: Reproduce Part 1 Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "file-download-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's set up basic components like Part 1\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "from data.download_data import download_apple_10k\n",
    "\n",
    "# Download Apple 10-K report (reuse from Part 1)\n",
    "file_path = download_apple_10k()\n",
    "print(f\"Downloaded: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process documents (same as Part 1)\n",
    "import torch\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Check device availability\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} pages\")\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zby29hyf2t",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš« Reproducing Part 1 Limitations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Part 1 approach: In-memory vector store\n",
    "memory_store = Qdrant.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # â† This data will be lost!\n",
    "    collection_name=\"part1_memory\"\n",
    ")\n",
    "\n",
    "print(\"âœ… In-memory vector store created\")\n",
    "print(f\"ğŸ“Š Current vector count: {memory_store.client.count(collection_name='part1_memory').count}\")\n",
    "\n",
    "# Display memory usage\n",
    "import psutil\n",
    "process = psutil.Process()\n",
    "memory_usage = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"ğŸ’¾ Current memory usage: {memory_usage:.1f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"âš ï¸ ACTION REQUIRED: Please restart the kernel now!\")\n",
    "print(\"   (Jupyter menu: Kernel â†’ Restart)\")\n",
    "print(\"   Then, run the cell below.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bsae935q29g",
   "metadata": {},
   "source": [
    "### âš ï¸ Kernel Restart Required\n",
    "\n",
    "**Perform the following steps now:**\n",
    "\n",
    "1. Click `Kernel â†’ Restart` in Jupyter menu\n",
    "2. After kernel restarts, **skip the cells above** and run only the cell below\n",
    "3. Verify if the data is truly lost!\n",
    "\n",
    "> ğŸ’¡ **Key Point**: The `:memory:` approach stores data only in Python process memory, so all data is completely lost when the kernel restarts (process terminates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1rnug3krtri",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” Verifying Data Persistence After Kernel Restart\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Connect to in-memory instance with new client\n",
    "# Note: Data from before kernel restart cannot be found\n",
    "client_after_restart = QdrantClient(location=\":memory:\")\n",
    "\n",
    "try:\n",
    "    # Attempt to retrieve previously created collection info\n",
    "    collection_info = client_after_restart.get_collection(collection_name=\"part1_memory\")\n",
    "    print(f\"âœ… Collection found! Vector count: {collection_info.points_count}\")\n",
    "    print(\"ğŸ’¡ If you see this message: You didn't restart the kernel, or you re-ran the cells above.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error occurred: Cannot access collection 'part1_memory'.\")\n",
    "    print(f\"   Reason: In-memory database was completely reset after kernel restart.\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "    print(\"ğŸ’¡ This is proof that the data is not persistent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 2: Enhanced Document Processing with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ea890-04f2-45d4-8467-2bfe1652ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "from data.download_data import download_apple_10k\n",
    "\n",
    "file_path = download_apple_10k()\n",
    "print(f\"Downloaded: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def enrich_documents_with_metadata(pdf_path):\n",
    "    \"\"\"Unlike Part 1, add rich metadata\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"ğŸ“š Loaded pages: {len(documents)}\")\n",
    "\n",
    "    project_root = Path().absolute().parent \n",
    "    \n",
    "    for doc in documents:\n",
    "        absolute_path = Path(doc.metadata['source'])\n",
    "        relative_path = absolute_path.relative_to(project_root.parent)\n",
    "        doc.metadata['source'] = str(relative_path)\n",
    "        \n",
    "    # Split into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Enrich metadata\n",
    "    enriched_chunks = []\n",
    "    for chunk in chunks:\n",
    "        page_num = chunk.metadata.get('page', 0)\n",
    "        content_lower = chunk.page_content.lower()\n",
    "        \n",
    "        # Automatic section classification (heuristic)\n",
    "        if any(keyword in content_lower for keyword in ['revenue', 'sales', 'net income']):\n",
    "            section = \"Financial Performance\"\n",
    "        elif any(keyword in content_lower for keyword in ['risk', 'uncertainty', 'factor']):\n",
    "            section = \"Risk Factors\"\n",
    "        elif any(keyword in content_lower for keyword in ['business', 'product', 'services']):\n",
    "            section = \"Business Overview\"\n",
    "        else:\n",
    "            section = \"Other\"\n",
    "        \n",
    "        # Part 2 key feature: Rich metadata\n",
    "        chunk.metadata.update({\n",
    "            'section': section,\n",
    "            'page_number': page_num,\n",
    "            'doc_type': '10-K',\n",
    "            'year': 2023,\n",
    "            'company': 'Apple Inc.',\n",
    "            'file_name': Path(pdf_path).name\n",
    "        })\n",
    "        enriched_chunks.append(chunk)\n",
    "    \n",
    "    return enriched_chunks\n",
    "\n",
    "# Generate metadata-enriched chunks\n",
    "enriched_chunks = enrich_documents_with_metadata(file_path)\n",
    "print(f\"âœ… Metadata enrichment complete: {len(enriched_chunks)} chunks\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Extract 'section' metadata from each chunk into a list\n",
    "sections = [chunk.metadata['section'] for chunk in enriched_chunks]\n",
    "\n",
    "# Count each section using Counter\n",
    "section_counts = Counter(sections)\n",
    "\n",
    "print(\"\\nğŸ“Š Chunk count by section (debugging):\")\n",
    "for section, count in section_counts.items():\n",
    "    print(f\"   - {section}: {count}\")\n",
    "\n",
    "# Check metadata example\n",
    "sample_chunk = enriched_chunks[10]\n",
    "print(f\"\\nğŸ“‹ Metadata example:\")\n",
    "for key, value in sample_chunk.metadata.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 3: Create Production Qdrant Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up persistent storage Qdrant collection\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# Connect to Qdrant client\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "print(\"ğŸ”— Qdrant server connection successful!\")\n",
    "\n",
    "# Create production-grade collection\n",
    "collection_name = \"apple_10k_production\"\n",
    "\n",
    "try:\n",
    "    # Delete existing collection if exists (for demo)\n",
    "    client.delete_collection(collection_name)\n",
    "    print(f\"ğŸ—‘ï¸ Existing collection '{collection_name}' deleted\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=384,  # all-MiniLM-L6-v2 dimension\n",
    "        distance=Distance.COSINE,  # Cosine similarity\n",
    "        # HNSW optimization parameters (Part 2 core settings)\n",
    "        hnsw_config={\n",
    "            \"m\": 16,  # Number of connections\n",
    "            \"ef_construct\": 100,  # Search depth during construction\n",
    "            \"full_scan_threshold\": 10000,  # Full scan threshold\n",
    "            \"on_disk\": False  # In-memory index (faster search)\n",
    "        }\n",
    "    ),\n",
    "    # Optimization: Automatic metadata indexing\n",
    "    optimizers_config={\n",
    "        \"deleted_threshold\": 0.2,\n",
    "        \"vacuum_min_vector_number\": 1000,\n",
    "        \"default_segment_number\": 0,\n",
    "        \"max_segment_size\": None,\n",
    "        \"memmap_threshold\": 1000000,\n",
    "        \"indexing_threshold\": 20000,\n",
    "        \"flush_interval_sec\": 5,\n",
    "        \"max_optimization_threads\": None\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"âœ… Production collection '{collection_name}' created!\")\n",
    "\n",
    "# Check collection info\n",
    "collection_info = client.get_collection(collection_name)\n",
    "print(f\"ğŸ“Š Vector dimension: {collection_info.config.params.vectors.size}\")\n",
    "print(f\"ğŸ“ Distance metric: {collection_info.config.params.vectors.distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Index Documents to Qdrant with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "\n",
    "def index_documents_to_qdrant(client, collection_name, chunks, embeddings):\n",
    "    \"\"\"Index documents to Qdrant with metadata\"\"\"\n",
    "    print(\"ğŸ”„ Starting document vectorization and Qdrant indexing...\")\n",
    "    \n",
    "    points = []\n",
    "    batch_size = 50\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Vectorization progress\")):\n",
    "        # Convert text to vector\n",
    "        vector = embeddings.embed_query(chunk.page_content)\n",
    "        \n",
    "        # Create Qdrant Point (ID, vector, metadata)\n",
    "        point = PointStruct(\n",
    "            id=str(uuid.uuid4()),  # Unique ID\n",
    "            vector=vector,\n",
    "            payload={\n",
    "                \"page_content\": chunk.page_content,\n",
    "                \"section\": chunk.metadata.get(\"section\"),\n",
    "                \"page_number\": chunk.metadata.get(\"page_number\"),\n",
    "                \"doc_type\": chunk.metadata.get(\"doc_type\"),\n",
    "                \"year\": chunk.metadata.get(\"year\"),\n",
    "                \"company\": chunk.metadata.get(\"company\"),\n",
    "                \"file_name\": chunk.metadata.get(\"file_name\")\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "        \n",
    "        # Upload in batches\n",
    "        if len(points) >= batch_size:\n",
    "            client.upsert(collection_name=collection_name, points=points)\n",
    "            points = []\n",
    "    \n",
    "    # Upload remaining points\n",
    "    if points:\n",
    "        client.upsert(collection_name=collection_name, points=points)\n",
    "    \n",
    "    print(f\"âœ… {len(chunks)} documents indexed!\")\n",
    "    \n",
    "    # Check collection statistics\n",
    "    collection_info = client.get_collection(collection_name)\n",
    "    print(f\"ğŸ“Š Stored vector count: {collection_info.points_count}\")\n",
    "    return collection_info.points_count\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Execute actual indexing\n",
    "indexed_count = index_documents_to_qdrant(\n",
    "    client, collection_name, enriched_chunks, embeddings\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ‰ Part 2 production vector database construction complete!\")\n",
    "print(f\"ğŸ“¦ Total {indexed_count} vectors permanently stored\")\n",
    "print(\"ğŸ”„ Data will persist even after notebook restart!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 5: Compare Search Performance (Part 1 vs Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Part 1 style search function\n",
    "def simple_vector_search(query, top_k=5):\n",
    "    \"\"\"Part 1 style: Simple search without metadata filtering\"\"\"\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    \n",
    "    result = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return result.points\n",
    "\n",
    "# Part 2 style search function\n",
    "def filtered_vector_search(query, section=None, page_range=None, top_k=5):\n",
    "    \"\"\"Part 2 style: Search with metadata filtering\"\"\"\n",
    "    from qdrant_client.models import Filter, FieldCondition, MatchValue, Range\n",
    "    \n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    \n",
    "    # Build filter conditions\n",
    "    filter_conditions = []\n",
    "    if section:\n",
    "        filter_conditions.append(\n",
    "            FieldCondition(key=\"section\", match=MatchValue(value=section))\n",
    "        )\n",
    "    if page_range:\n",
    "        filter_conditions.append(\n",
    "            FieldCondition(key=\"page_number\",\n",
    "                         range=Range(gte=page_range[0], lte=page_range[1]))\n",
    "        )\n",
    "    \n",
    "    query_filter = Filter(must=filter_conditions) if filter_conditions else None\n",
    "    \n",
    "    result = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        query_filter=query_filter,\n",
    "        limit=top_k,\n",
    "        score_threshold=0.4,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return result.points\n",
    "\n",
    "# Run comparison test\n",
    "test_query = \"What was Apple's total revenue in 2023?\"\n",
    "\n",
    "print(\"ğŸ” Search Method Comparison Test\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {test_query}\")\n",
    "\n",
    "# Part 1 style search\n",
    "print(\"\\nâŒ Part 1 Style: Simple Vector Search\")\n",
    "start_time = time.time()\n",
    "simple_results = simple_vector_search(test_query)\n",
    "simple_time = time.time() - start_time\n",
    "\n",
    "print(f\"â±ï¸ Search time: {simple_time:.3f}s\")\n",
    "print(f\"ğŸ“Š Result count: {len(simple_results)}\")\n",
    "\n",
    "for i, result in enumerate(simple_results, 1):\n",
    "    section = result.payload.get('section', 'Unknown')\n",
    "    page = result.payload.get('page_number', 'Unknown')\n",
    "    score = result.score\n",
    "    print(f\"   {i}. Section:{section}, Page:{page}, Score:{score:.3f}\")\n",
    "\n",
    "# Part 2 style search\n",
    "print(\"\\nâœ… Part 2 Style: Metadata Filtered Search\")\n",
    "start_time = time.time()\n",
    "filtered_results = filtered_vector_search(\n",
    "    test_query,\n",
    "    section=\"Financial Performance\",  # Search only financial section\n",
    ")\n",
    "filtered_time = time.time() - start_time\n",
    "\n",
    "print(f\"â±ï¸ Search time: {filtered_time:.3f}s\")\n",
    "print(f\"ğŸ“Š Result count: {len(filtered_results)}\")\n",
    "\n",
    "for i, result in enumerate(filtered_results, 1):\n",
    "    section = result.payload.get('section', 'Unknown')\n",
    "    page = result.payload.get('page_number', 'Unknown')\n",
    "    score = result.score\n",
    "    print(f\"   {i}. Section:{section}, Page:{page}, Score:{score:.3f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Performance Comparison:\")\n",
    "print(f\"   Speed: {((simple_time - filtered_time) / simple_time * 100):+.1f}% change with filtering\")\n",
    "print(f\"   Accuracy: Filtering yields more relevant results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 6: Complete RAG Pipeline with Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def ask_rag_with_filter(query, section=None, page_range=None):\n",
    "    \"\"\"RAG Q&A with filtering functionality\"\"\"\n",
    "    # 1. Filtered vector search\n",
    "    search_results = filtered_vector_search(query, section, page_range)\n",
    "    \n",
    "    if not search_results:\n",
    "        return \"No relevant documents found.\", []\n",
    "    \n",
    "    # 2. Build context\n",
    "    contexts = []\n",
    "    sources = []\n",
    "    \n",
    "    for result in search_results:\n",
    "        content = result.payload.get('page_content', '')\n",
    "        metadata = {\n",
    "            'section': result.payload.get('section'),\n",
    "            'page': result.payload.get('page_number'),\n",
    "            'score': result.score\n",
    "        }\n",
    "        contexts.append(content)\n",
    "        sources.append(metadata)\n",
    "    \n",
    "    # 3. Build LLM prompt\n",
    "    context_text = \"\\n\\n\".join(contexts)\n",
    "    prompt = f\"\"\"Answer the question accurately based on the following Apple 2023 10-K report information:\n",
    "\n",
    "Reference materials:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: Please provide an answer with specific numbers based on the provided materials.\"\"\"\n",
    "    \n",
    "    # 4. Generate LLM response\n",
    "    response = ollama.chat(\n",
    "        model='llama3.1:8b',\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content'], sources\n",
    "\n",
    "# Test with various questions\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What was Apple's total revenue in 2023?\",\n",
    "        \"section\": \"Financial Performance\",\n",
    "        \"page_range\": (20, 60)\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the main risk factors Apple faces?\",\n",
    "        \"section\": \"Risk Factors\",\n",
    "        \"page_range\": (5, 20)\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are Apple's main products and services?\",\n",
    "        \"section\": \"Business Overview\",\n",
    "        \"page_range\": (1, 30)\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ Part 2 Production RAG System Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{i}. Question: {test_case['query']}\")\n",
    "    print(f\"   Filter: Section={test_case['section']}, Pages={test_case['page_range']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    answer, sources = ask_rag_with_filter(**test_case)\n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   â±ï¸ Response time: {response_time:.2f}s\")\n",
    "    print(f\"   ğŸ“„ Referenced documents: {len(sources)}\")\n",
    "    print(f\"   ğŸ“ Answer: {answer[:200]}...\")\n",
    "    \n",
    "    # Display source information\n",
    "    print(\"   ğŸ“š Reference sources:\")\n",
    "    for j, source in enumerate(sources[:2], 1):\n",
    "        print(f\"      {j}. Section:{source['section']}, Page:{source['page']}, Score:{source['score']:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… All tests complete! Part 2 production RAG system is functioning properly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 7: Data Persistence Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data_persistence():\n",
    "    \"\"\"Verify data persistence\"\"\"\n",
    "    print(\"ğŸ” Data Persistence Verification\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check collection existence\n",
    "    collections = client.get_collections()\n",
    "    collection_names = [col.name for col in collections.collections]\n",
    "    \n",
    "    if collection_name in collection_names:\n",
    "        print(f\"âœ… Collection '{collection_name}' exists\")\n",
    "        \n",
    "        # Check stored data count\n",
    "        collection_info = client.get_collection(collection_name)\n",
    "        print(f\"ğŸ“Š Stored vector count: {collection_info.points_count}\")\n",
    "        \n",
    "        # Sample search test\n",
    "        sample_results = simple_vector_search(\"Apple revenue\", top_k=3)\n",
    "        print(f\"ğŸ” Sample search results: {len(sample_results)}\")\n",
    "        \n",
    "        # Check metadata\n",
    "        if sample_results:\n",
    "            sample = sample_results[0]\n",
    "            print(\"ğŸ“‹ Sample metadata:\")\n",
    "            for key, value in sample.payload.items():\n",
    "                if key != 'page_content':\n",
    "                    print(f\"   {key}: {value}\")\n",
    "        \n",
    "        print(\"\\nğŸ‰ Data persistence verification complete!\")\n",
    "        print(\"ğŸ’¡ This data will persist even after notebook kernel restart.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ Collection '{collection_name}' not found\")\n",
    "        return False\n",
    "\n",
    "# Run verification\n",
    "persistence_ok = verify_data_persistence()\n",
    "\n",
    "if persistence_ok:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ† Part 2 Achievements Summary\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ… Docker Qdrant server deployment\")\n",
    "    print(\"âœ… Metadata-rich document indexing\")\n",
    "    print(\"âœ… Production-grade vector collection creation\")\n",
    "    print(\"âœ… Advanced metadata filtering search\")\n",
    "    print(\"âœ… Persistent storage data retention\")\n",
    "    print(\"âœ… Performance improvement and accuracy enhancement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Step 8: Identify Part 2 Limitations and Part 3 Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš¨ Testing Part 2 System Limitations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Questions that show limitations of dense search\n",
    "limitation_queries = [\n",
    "    \"Find content related to AAPL stock symbol\",  # Exact keyword matching needed\n",
    "    \"2023 fiscal year Q4 quarterly results\",        # Multi-language search\n",
    "    \"Tim Cook CEO leadership\",                       # Proper noun search\n",
    "    \"iPhone 15 Pro Max specifications\"               # New product info (may not be in 10K)\n",
    "]\n",
    "\n",
    "print(\"\\nâŒ Cases where dense search alone has limitations:\")\n",
    "\n",
    "for i, query in enumerate(limitation_queries, 1):\n",
    "    print(f\"\\n{i}. Question: \\\"{query}\\\"\")\n",
    "    \n",
    "    # Search with current system\n",
    "    results = simple_vector_search(query, top_k=3)\n",
    "    \n",
    "    if results and results[0].score > 0.7:\n",
    "        print(f\"   Result: {len(results)} found (best score: {results[0].score:.3f})\")\n",
    "        print(\"   âœ… Dense search found appropriate results\")\n",
    "    else:\n",
    "        score = results[0].score if results else 0\n",
    "        print(f\"   Result: {len(results)} found (best score: {results[0].score:.3f})\")\n",
    "        print(\"   âŒ Dense search struggles to find highly relevant results\")\n",
    "        print(\"   ğŸ’¡ Keyword-based search (BM25) might be more appropriate\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”® Problems to Solve in Part 3\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ Hybrid Search (Dense + Sparse)\")\n",
    "print(\"   - Combine semantic search + exact keyword matching\")\n",
    "print(\"   - Generate sparse vectors with SPLADE model\")\n",
    "print(\"   - Combine BM25 and vector search scores\")\n",
    "print()\n",
    "print(\"ğŸ¯ Query Expansion and Rewriting\")\n",
    "print(\"   - Transform user questions to be more search-friendly\")\n",
    "print(\"   - Expand synonyms and abbreviations\")\n",
    "print(\"   - Multiple query strategies\")\n",
    "print()\n",
    "print(\"ğŸ¯ Re-ranking\")\n",
    "print(\"   - More sophisticated re-ranking of initial search results\")\n",
    "print(\"   - Utilize cross-encoder models\")\n",
    "print(\"   - Comprehensive evaluation of metadata and content\")\n",
    "print()\n",
    "print(\"ğŸ’« From Part 2 to Part 3...\")\n",
    "print(\"   Now that we've built a solid foundation with dense search,\")\n",
    "print(\"   let's combine it with sparse search to create the perfect RAG!\")\n",
    "\n",
    "print(f\"\\nğŸŠ Part 2 complete! See you in Part 3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Optional: Server Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def stop_qdrant_server():\n",
    "    \"\"\"Stop Qdrant server and clean up resources with Docker Compose\"\"\"\n",
    "    print(\"ğŸ›‘ Stopping Qdrant server...\")\n",
    "    \n",
    "    try:\n",
    "        # Stop Docker Compose services\n",
    "        result = subprocess.run(\n",
    "            [\"docker-compose\", \"down\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=\".\"\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Qdrant server stopped!\")\n",
    "            print(\"ğŸ“¦ Docker containers have been stopped.\")\n",
    "            \n",
    "            # Confirm data preservation\n",
    "            print(\"\\nğŸ’¾ Data Preservation Status:\")\n",
    "            print(\"   âœ… Named volume data is preserved\")\n",
    "            print(\"   âœ… Data can be recovered on next 'docker-compose up -d' run\")\n",
    "        \n",
    "        else:\n",
    "            print(\"âš ï¸ Some errors occurred during server shutdown (may be normal)\")\n",
    "            print(f\"   Output: {result.stderr}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"âš ï¸ docker-compose command not found.\")\n",
    "        print(\"   Please run 'docker-compose down' manually.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Unexpected error: {e}\")\n",
    "\n",
    "def show_cleanup_options():\n",
    "    \"\"\"Guide user on cleanup options\"\"\"\n",
    "    print(\"ğŸ§¹ Cleanup Options After Practice\")\n",
    "    print(\"=\" * 40)\n",
    "    print()\n",
    "    print(\"1ï¸âƒ£ Stop server only (preserve data):\")\n",
    "    print(\"   stop_qdrant_server()  # â† Run this function\")\n",
    "    print(\"   ğŸ“‹ Effect: Only stop Docker container, keep data\")\n",
    "    print()\n",
    "    print(\"2ï¸âƒ£ Complete cleanup (delete data):\")\n",
    "    print(\"   !docker-compose down -v\")\n",
    "    print(\"   ğŸ“‹ Effect: Completely delete all data and volumes\")\n",
    "    print()\n",
    "    print(\"3ï¸âƒ£ Leave as is:\")\n",
    "    print(\"   ğŸ“‹ Effect: Server continues running (can do other experiments)\")\n",
    "    print(\"   ğŸ’¡ Using port 6333 in background\")\n",
    "    print()\n",
    "    print(\"ğŸ¯ Recommendations:\")\n",
    "    print(\"   - Learning purpose: Option 1ï¸âƒ£ (preserve data, stop server only)\")\n",
    "    print(\"   - Complete cleanup: Option 2ï¸âƒ£ (free disk space)\")\n",
    "    print(\"   - Continue experiments: Option 3ï¸âƒ£ (test other questions)\")\n",
    "\n",
    "# Show cleanup options\n",
    "show_cleanup_options()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ’¡ To stop the server, uncomment the cell below and run it\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server shutdown execution (optional)\n",
    "# Uncomment and run the line below to stop the Qdrant server\n",
    "\n",
    "# stop_qdrant_server()\n",
    "\n",
    "print(\"Uncomment and run the function above to stop the server.\")\n",
    "print(\"ğŸ’¾ Data is safely preserved in Docker named volume!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ğŸ‰ **Part 2 Complete! Production RAG System Successfully Built!**\n",
    "\n",
    "### Key Improvements Compared to Part 1:\n",
    "\n",
    "| Aspect | Part 1 | Part 2 |\n",
    "|--------|---------|--------|\n",
    "| **Data Storage** | Memory (volatile) | Docker persistent storage |\n",
    "| **Search Method** | Simple vector search | Metadata filtering + vector |\n",
    "| **Scalability** | Memory constrained | Production environment ready |\n",
    "| **Data Persistence** | Lost on restart | Retained through server restart |\n",
    "| **Search Accuracy** | Basic level | Greatly improved with filtering |\n",
    "\n",
    "### Core Achievements:\n",
    "- âœ… Docker-based production-grade Qdrant server deployment\n",
    "- âœ… Metadata-rich document processing and indexing  \n",
    "- âœ… HNSW optimization parameters applied\n",
    "- âœ… Sophisticated filtering search by section and page\n",
    "- âœ… Data persistence secured with permanent storage\n",
    "- âœ… Significantly improved performance and accuracy over Part 1\n",
    "\n",
    "### Next Steps (Part 3 Preview):\n",
    "**One Step Further with Hybrid Search!**\n",
    "- Dense search (Part 2) + Sparse search (BM25)\n",
    "- Perfect combination of semantic search and exact keyword matching\n",
    "- More sophisticated search results and re-ranking techniques\n",
    "\n",
    "Now that we've built a solid foundation in Part 2, we'll implement more sophisticated hybrid search in Part 3! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}